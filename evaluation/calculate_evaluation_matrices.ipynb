{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# dowload the dataset from google drive \n",
        "! gdown --id 1CX3x0YmRmDQivhjfgB0kVzfXzVql7Axj\n",
        "\n",
        "# unzip the dataset \n",
        "!unzip /content/stage1_train.zip\n",
        "\n",
        "# dowload the basic functions\n",
        "! gdown --id 1ZFtLIVLDyEX_AoK22peWdszOanvKxGyi\n",
        "! gdown --id 1YF2GqFbLPdIsPCODZ4fA3mWaprPuNjuR"
      ],
      "metadata": {
        "id": "01U12aMwQrzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ4KWjfMQflm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from skimage import io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import data_preprocess as dp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTM_boqNQ4jH",
        "outputId": "65eae1bf-08cd-4c3c-9654-9aa1bcbc570b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Access to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_coyOO65Q5Pl",
        "outputId": "1f7f9057-a452-42ea-a6b8-bd83350357b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Nuclie_data(Dataset):\n",
        "        def __init__(self,path):\n",
        "            self.path = path\n",
        "            self.folders = os.listdir(path)\n",
        "            self.transforms = get_transforms(0.5, 0.5)\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.folders)\n",
        "              \n",
        "        \n",
        "        def __getitem__(self,idx):\n",
        "            image_folder = os.path.join(self.path,self.folders[idx],'images/')\n",
        "            mask_folder = os.path.join(self.path,self.folders[idx],'masks/')\n",
        "            image_path = os.path.join(image_folder,os.listdir(image_folder)[0])\n",
        "            \n",
        "            #convert the RGBA into RGB\n",
        "            img = io.imread(image_path)[:,:,:3].astype('float32')\n",
        "            img = transform.resize(img,(128,128))\n",
        "\n",
        "            masks=[]\n",
        "            for mskName in os.listdir(mask_folder):\n",
        "                vesMask = (cv2.imread(mask_folder+'/'+mskName, 0) > 0).astype(np.uint8)  # Read vesse instance mask\n",
        "                vesMask=cv2.resize(vesMask,[128,128],cv2.INTER_NEAREST)\n",
        "                masks.append(vesMask)\n",
        "            \n",
        "            #mask = get_mask(mask_folder, 128, 128 ).astype('float32')\n",
        "            boxes = get_bounding_box(mask_folder, 128, 128 )\n",
        "          \n",
        "            # convert everything into a torch.Tensor\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "            #img = torch.as_tensor(img, dtype=torch.float32)\n",
        "            image_id = torch.tensor([idx])\n",
        "            \n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "            \n",
        "            #augmentation\n",
        "            augmented = self.transforms(image=img)\n",
        "            img = augmented['image']\n",
        "            #mask = augmented['mask']\n",
        "            num_objs = len(boxes)\n",
        "            \n",
        "            # suppose all instances are not crow\n",
        "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "            \n",
        "            target = {}\n",
        "            target[\"boxes\"] = boxes \n",
        "\n",
        "            #there is only one class \n",
        "            target[\"labels\"] =  torch.ones((num_objs,), dtype=torch.int64)   # there is only one class\n",
        "            target[\"masks\"] = masks\n",
        "            target[\"image_id\"] = image_id\n",
        "            target[\"area\"] = area\n",
        "            target[\"iscrowd\"] = iscrowd\n",
        "            \n",
        "            return (img,target)"
      ],
      "metadata": {
        "id": "lKt0UPEfQ7dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##get the bounding box for each individual mask \n",
        "def get_bounding_box(mask_folder,IMG_HEIGHT, IMG_WIDTH):\n",
        "    boxes = []\n",
        "    #masks = []\n",
        "    for mask_ in os.listdir(mask_folder):\n",
        "            mask_ = io.imread(os.path.join(mask_folder,mask_)).astype('float32')\n",
        "            mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))\n",
        "            ###get the bounding box\n",
        "            pos = np.where(mask_)\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            #masks.append(mask_)\n",
        "              \n",
        "    return boxes"
      ],
      "metadata": {
        "id": "MoQZCY2hRRZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms(mean, std):\n",
        "            list_transforms = []\n",
        "            \n",
        "            list_transforms.extend(\n",
        "                    [\n",
        "                HorizontalFlip(p=0), # only horizontal flip as of now\n",
        "                    ])\n",
        "            list_transforms.extend(\n",
        "                    [\n",
        "            Normalize(mean=mean, std=std, p=1),\n",
        "            ToTensorV2(),\n",
        "                    ])\n",
        "            list_trfms = Compose(list_transforms)\n",
        "            return list_trfms"
      ],
      "metadata": {
        "id": "Rnbj3wqVRUgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the datad\n",
        "data_dir = '/content/stage1_train/'\n",
        "data = Nuclie_data(data_dir)"
      ],
      "metadata": {
        "id": "faYizJjYRWys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data size\n",
        "print(data.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RsI4ISCRaO_",
        "outputId": "b7d28e4b-ec8d-42a5-ae5b-0f531b985b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Semantic Segmentation"
      ],
      "metadata": {
        "id": "hu_YhL-Yp05J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models.segmentation\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
      ],
      "metadata": {
        "id": "6XwP1mRTRb2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_semantic(num_classes):\n",
        "    #load the pre-trained model \n",
        "    model=torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) \n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
        "    model.roi_heads.box_predictor=FastRCNNPredictor(in_features,num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "spTzxPBtp6jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## get trained models\n",
        "model_path = \"/content/MyDrive/MyDrive/trained_model/Combined_masks/\"\n",
        "model_list = []\n",
        "# Iterate directory\n",
        "for file in os.listdir(model_path):\n",
        "    # check only text files\n",
        "    if file.endswith('.torch'):\n",
        "        model_list.append(file)\n",
        "model_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WCDwnpTqjUy",
        "outputId": "263b5413-a776-4e2b-8347-63063a96ff9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000_augmentation_batch2.torch',\n",
              " '200_augmentation_batch4.torch',\n",
              " '400_augmentation_batch4.torch',\n",
              " '600_augmentation_batch4.torch',\n",
              " '800_augmentation_batch4.torch',\n",
              " '1000_augmentation_batch4.torch',\n",
              " '200_augmentation_batch6.torch',\n",
              " '400_augmentation_batch6.torch',\n",
              " '600_augmentation_batch6.torch',\n",
              " '800_augmentation_batch6.torch',\n",
              " '1000_augmentation_batch6.torch',\n",
              " '200_augmentation_batch8.torch',\n",
              " '400_augmentation_batch8.torch',\n",
              " '600_augmentation_batch8.torch',\n",
              " '800_augmentation_batch8.torch',\n",
              " '1000_augmentation_batch8.torch',\n",
              " '200_augmentation_batch10.torch',\n",
              " '400_augmentation_batch10.torch',\n",
              " '600_augmentation_batch10.torch',\n",
              " '800_augmentation_batch10.torch',\n",
              " '1000_augmentation_batch10.torch',\n",
              " '1000_augmentation_batch1.torch']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for trained_model in model_list:\n",
        "\n",
        "    model = get_model_semantic(2)\n",
        "\n",
        "    print(\"Evaluating the model:\", trained_model)\n",
        "    model.load_state_dict(torch.load(os.path.join(model_path, trained_model)))\n",
        "    model.eval()\n",
        "    model.to(device)# move model to the right device\n",
        "\n",
        "    evaluator = dp.evaluate_IoU(data, model, range(581, 670), device)\n",
        "    print(\" \")\n",
        "    print(\" \")\n"
      ],
      "metadata": {
        "id": "2ky3VcZStOMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance segamentation "
      ],
      "metadata": {
        "id": "P_5SNiTlqkWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MIKwTvxuRoz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/MyDrive/MyDrive/trained_model/individual_mask/\"\n",
        "model_list = []\n",
        "# Iterate directory\n",
        "for file in os.listdir(model_path):\n",
        "    # check only text files\n",
        "    if file.endswith('.torch'):\n",
        "        model_list.append(file)\n",
        "print(model_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8A58CDaToV4",
        "outputId": "39c42c2e-dfc4-41dd-d824-1a08f2ee07b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['500_individal_batch4.torch', '1000_individal_batch4.torch', '1500_individal_batch4.torch', '2000_individal_batch4.torch', '500_individal_batch6.torch', '1000_individal_batch6.torch', '1500_individal_batch6.torch', '2000_individal_batch6.torch', '500_individal_batch8.torch', '1000_individal_batch8.torch', '1500_individal_batch8.torch', '2000_individal_batch8.torch', '500_individal_batch10.torch', '1000_individal_batch10.torch', '1500_individal_batch10.torch', '2000_individal_batch10.torch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ErlHrBD_bgl",
        "outputId": "a69e6195-932d-4a78-89db-e56a20dba3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['500_individal_batch4.torch',\n",
              " '1000_individal_batch4.torch',\n",
              " '1500_individal_batch4.torch',\n",
              " '2000_individal_batch4.torch',\n",
              " '500_individal_batch6.torch',\n",
              " '1000_individal_batch6.torch',\n",
              " '1500_individal_batch6.torch',\n",
              " '2000_individal_batch6.torch',\n",
              " '500_individal_batch8.torch',\n",
              " '1000_individal_batch8.torch',\n",
              " '1500_individal_batch8.torch',\n",
              " '2000_individal_batch8.torch',\n",
              " '500_individal_batch10.torch',\n",
              " '1000_individal_batch10.torch',\n",
              " '1500_individal_batch10.torch',\n",
              " '2000_individal_batch10.torch']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for trained_model in model_list:\n",
        "\n",
        "    model = get_model_instance_segmentation(2)\n",
        "\n",
        "    print(\"Evaluating the model:\", trained_model)\n",
        "    model.load_state_dict(torch.load(os.path.join(model_path, trained_model)))\n",
        "    model.eval()\n",
        "    model.to(device)# move model to the right device\n",
        "\n",
        "    evaluator = dp.evaluate_IoU(data, model, range(581, 670), device)\n",
        "    print(\" \")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "7J2DrhoWUUyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}